
# ğŸ§  TokenOptimizer â€“ Developer Engineering Overview

**TokenOptimizer** is an LLM observability tool that helps developers monitor token usage and optimize model spend. It works with any GenAI app using OpenAI, Anthropic, Mistral, or other API-accessible models.

---

## ğŸ” What It Does

- Tracks LLM API calls from your app
- Logs token usage, latency, and model used
- Calculates estimated cost based on latest pricing data
- Displays this usage in a clean, live web dashboard
- (Future) Suggests cheaper model alternatives based on real usage patterns

---

## ğŸ› ï¸ How It Works (MVP)

### 1. ğŸ§© Python SDK

Drop-in wrapper around your LLM API calls:

```python
from tokenoptimizer import tracked_completion

response = tracked_completion(
    model="claude-3-haiku",
    messages=[{"role": "user", "content": "Summarize this PDF"}],
    endpoint_name="default"  # Optional task tag
)
```

**Key behavior:**
- Measures latency
- Extracts `model`, `prompt_tokens`, `completion_tokens`, `total_tokens`
- Calculates estimated request cost using model pricing table
- Sends anonymized metadata to TokenOptimizer backend (`POST /log`)
- Does **not** store prompt contents
- Adds **no latency** or logic impact to your app

#### âœ³ï¸ Tagging (Important)

- `endpoint_name` helps group usage by feature (e.g., `"summarization"`, `"chatbot"`)
- If not provided, we tag usage as `"default"`
- In the dashboard, usage will be broken down by these tags

---

### 2. ğŸŒ Dashboard (React + Supabase)

Accessible at:

`https://dashboard.tokenoptimizer.com` *(coming soon)*

**MVP Features:**
- Total token usage over time (line graph)
- Cost breakdown by model and feature
- Pie chart for model distribution
- Table view for usage by `endpoint_name`
- Latency tracking per model
- Logs view (timestamp, model, latency, tokens, cost)

---

### 3. ğŸ’° Model Pricing Integration

We use an internal `model_pricing` table in Supabase:

```sql
CREATE TABLE model_pricing (
  model TEXT PRIMARY KEY,
  input_price NUMERIC,
  output_price NUMERIC,
  last_updated TIMESTAMP
);
```

This is queried during ingestion to compute:
- `input_cost = prompt_tokens / 1000 * input_price`
- `output_cost = completion_tokens / 1000 * output_price`
- `total_cost = input_cost + output_cost`

**This approach allows us to update pricing without hardcoding.**

---

## ğŸ§° Tech Stack

| Component | Tech |
|----------|------|
| Frontend Dashboard | React (Vercel) |
| Backend API | Flask (Render) |
| Database | Supabase (Postgres) |
| SDK | Python module (importable or pip-install) |

---

## ğŸ›¡ï¸ Privacy & Performance Principles

- âŒ No prompt content is stored
- âœ… No PII is logged by default
- âœ… No impact on application logic
- âœ… No added latency
- âœ… Secure transmission with HTTPS
- âœ… Minimal dev effort to integrate

---

## ğŸš€ Near-Term Roadmap

- âœ… SDK to log token usage
- âœ… Flask API to ingest and store logs
- âœ… React dashboard to visualize token usage
- âœ… Cost mapping with live pricing DB
- ğŸ”œ Feature grouping (task suggestions, UI labels)
- ğŸ”œ Benchmark runner for silent model comparisons

---

## ğŸ“¦ Install (Coming)

```bash
pip install tokenoptimizer
```

---

## ğŸ“ Project Layout

```
/backend      â†’ Flask API + Supabase integration
/frontend     â†’ React dashboard UI
/sdk          â†’ Python module to track LLM usage
```

---

## ğŸ§  Summary

**TokenOptimizer** is your "LLM spend dashboard" â€” built for devs, with zero risk, and maximum insight.
